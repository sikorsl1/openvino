# OpenVINO Workflow {#openvino_workflow}


@sphinxdirective

.. toctree::
   :maxdepth: 1
   :hidden:

   Model Preparation <openvino_docs_model_processing_introduction>
   Model Optimization and Compression <openvino_docs_model_optimization_guide>
   Running and Deploying Inference <openvino_docs_deployment_guide_introduction>
   

| :doc:`Model Preparation <openvino_docs_model_processing_introduction>`
| With Model Downloader and Model Optimizer guides, you will learn to download pre-trained models and convert them for use with OpenVINOâ„¢. You can use your own models or choose some from a broad selection provided in the Open Model Zoo.

| :doc:`Model Optimization and Compression <openvino_docs_model_optimization_guide>`
| In this section you will find out how to optimize a model to achieve better inference performance. It describes multiple optimization methods for both the training and post-training stages. 

| :doc:`Deployment <openvino_docs_deployment_guide_introduction>`
| This section explains the process of deploying your own inference application using either OpenVINO Runtime or OpenVINO Model Server. It describes how to run inference which is the most basic form of deployment and the quickest way of launching inference.

@endsphinxdirective